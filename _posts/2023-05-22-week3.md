# The end is nye!

## Let's start with where I am at currently!
I have completed most of the assignment so far, Q3 was a worry to me at first as I thought I would have to develop my own ML technique, such as a CNN from scratch. However, I decided to use the same method as Q2 to see what my accuracy would be, and with the standard resnet18 in the *vision_learner*, I managed to get an accuracy of 95%! So, this post will mainly discuss the hyperparameters within the *vision_learner* model. As an aside, upon some initial research, I found an existing paper - written by Jordan J. Bird and Ahmad Lotfi - discussing the CIFAKE dataset, along with an in depth analysis of the dataset using a CNN with hyperparameter tuning. You can find this [here](https://arxiv.org/pdf/2303.14126.pdf)

## *vision_learner* - what can I do with it?
Using the fastai documentation, we can see the following hyperparameters that I would consider to be the most important for general use cases:
| **Hyperparameter** | **Description** |
|-|-|
| `arch` | Specifies the architecture of the CNN model. |
| `lr` | Specifies the step size at which the optimiser adjusts the model's weights during training |
| `loss_func` | Specifies the loss function to be used for training the model. |
| `opt_func` | Specifies the optimisation algorithm to be used for training the model. |

### `arch`
This specifies the structure and configuration of the CNN, including the number and type of layers, the connectivity between layers, and any specific architectural variations. Some existing solutions within Fastai include:
- *ResNet* : Residual Networks are deep CNN architectures that introduced the concept of residual connections. Some examples include resnet18, resnet34, resnet50 and resnet152. The number represents the number of layers in the neural network.
- *DenseNet* : A ensely connected CNN architecture where each layer is directly connected to every other layer in a feed-forward fashion. DenseNet variations include densenet121, densenet161, densenet169, and densenet201.

### `lr`
It represents a starting learning rate for training the model. However, it's important to note that the choice of an optimal learning rate can greatly affect model performance and training dynamics. It often requires experimentation to find the best learning rate for a specific task and model architecture. You can also use *lr_find* to find an appropriate learning rate before training your model. A common starting point for learning rate is either 0.1 or 0.01.

### `loss_func`
Refers to the loss function used to calculate the loss during training and evaluation of the model. The loss function measures the discrepancy between the predicted output of the model and the true target values, allowing the model to learn and optimize its parameters. Common example have been discussed in a previous post.

### `opt_func`
This is used to specify the optimizer function to be used during training. The optimizer is responsible for updating the model's parameters based on the calculated gradients. Some examples include:
- *Stochastic Gradient Descent (SGD)*: The SGD optimizer performs gradient descent with a specified learning rate. It is a widely used optimizer for training neural networks.
- *Adam*: The Adam optimizer combines ideas from adaptive learning rates and momentum. It adapts the learning rate for each parameter based on past gradients and maintains separate learning rates for each parameter. It is also the default for *vision_learner*.
- *RMSProp* The RMSProp optimizer also adjusts the learning rate for each parameter based on past gradients. It uses a moving average of squared gradients to normalize the parameter updates.

This information was curated from the Fastai website, if you would like to find out more, click [here](https://medium.com/swlh/resnets-densenets-unets-6bbdbcfdf010#:~:text=Dense%20Net%20is%20the%20concept,remains%20in%20the%20final%20output.)
