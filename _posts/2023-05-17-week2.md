# Lots of data!

## But first, a review of where I am at currently
Okay, so it's been about two weeks since the release of Assignment 3, and I am enjoying it for the most part.
Some hiccups that have been happening:
- Codespace CPU is ridiculously slow
- Google Collab isn't very collaborative, unless I am just missing something
- Trying to acquire the CIFAKE images directly from Kaggle is very tricky, an easy solution I used instead was to just download and upload the files directly

It seems the image downloading issues I had last week are following on with a lot of other students currently, so I am glad I managed to sort out my images and don't have to download any more.

Lastly, I have found no information about 

## Wait? I need 3 datasets?
That's right! What I've learnt is that to be following good practice, you should be using 3 datasets: Training, Validation and Testing. Let's start by defining each:

**Training Data:**
This is the actual dataset that is used to tran the model, for the case of our vision_learner, this trains the weights and biases as a pre-existing Neural Network is used. The model 'sees' and 'learns' from this data.

**Validation Data:**
This is used to frequently evaluate the model so that the model hyperparameters can be fine-tuned. In our case, we use *fine_tune* to fine tune the model using the validation data over a specific number of epochs. Hence, the model occasionally 'sees' the data, but doesn't 'learn' from it. So, the validation set affects a model, but only indirectly, if we don't fine tune our model, then the validation dataset acts the same as a test set, hence the confusion between the two.

**Testing Data:**
This provides an unbiased evaluation of a final model fit. We only use this data once the model is completely trained (including fine tuning).

### But, what about the split?
Ah, an excellent question! This depends on 2 things: the total number of samples in the data and the model itself that is being trained.

![dataset split](/images/data_split.png)

Some models need a ton of data to train on, so then you'd naturally create a larger training data set. An example of this is the CIFAKE data, which has 100 000 images to train on. Another case is if you have a model with no hyperparameters or ones that cannot be easily tuned, meaning you wouldn't need a validation dataset on it.

In saying all this, a general split is to have 60:20:20 or 80:10:10 for training, validation and test respectively.

For more information, follow the link [here](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7)
