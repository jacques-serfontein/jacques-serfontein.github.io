# The confusion around confusion matrices

## A small aside regarding issues that I have found this week.

An issue that cropped up this week was that my existing Codespaces would not successfully run methods that involved connecting to the internet. After testing with both my own and UQ internet, the only solution I could find was to delete my existing Codespace and create a new one.

## But first, why lose it over loss functions?

Loss functions can be a bit daunting, however fastai is great in the fact that the detection model "vision_learner" (used in Q2) allows for a "loss_func" parameter to be passed through.
Some of the built-in loss functions include:
- CrossEntropyLoss
- CrossEntropyLossFlat
- FocalLossFlat
- FocalLoss
- BCEWithLogitsLossFlat
- BCELossFlat
- MSELossFlat
- L1LossFlat
- LabelSmoothingCrossEntropy
- etc.

Consulting with ChatGPT, as well as researching online, the recommended loss function for a multi-class classification problem (Q2) would be CrossEntropyLoss. ChatGPT states:
"This loss function is commonly used for multi-class classification problems, and is especially suited for cases where the model needs to predict the probability distribution over multiple classes. Using CrossEntropyLoss is a good choice for multi-class classification problems because it takes into account both the correctness of the predicted class and the confidence of the prediction."

Lucky for me, the default loss function used for vision_learner is this exact loss_func. Thus, no adjustments need to actually be made to the existing code from 00 for the best result.

The website I used to gain a greater understanding for multi-class classification with fastai can be found [here](https://medium.com/analytics-vidhya/fastai-multi-label-classification-chapter-6-d28f3c9ed4f5#:~:text=In%20fastai%20we%20do%20not,BCEWithLogitsLoss%20by%20default.)

## Back on track to confusion
Essentially, a confusion matrix is a performance measurement for ML classification, where there are 2 or more classes. The easiest way to explain it, is by showing you:

![confusion_matrix](https://github.com/jacques-serfontein/jacques-serfontein.github.io/assets/88257966/2cb66fbb-aa9d-4a90-91de-14db55188e69)

Using an example of a 'bird or not' classifier, the matrix can be interpreted by:

| **Cell Item** | **Interpretation** | **Example** |
|-|-|-|
| TP = True Positive | You predicted positive and it's true | The classifier predicts an image is a bird, and it is |
| TN = True Negative | You predicted negative and it's true | The classifier predicts an image is not a bird, and it is not |
| FP = False Positive | You predicted positive and it's false | The classifier predicts an image is a bird, but it is not |
| FN: False Negative | You predicted negative and it's false | The classifier predicts and image is not a bird, and it is |

As you can tell, it's not that tricky to interpret! It's also very useful to note that you can get the accuracy that a classifier has on data by summing the diagonals of the matrix and dividing by the total data count! Click [here](https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62) if you want to learn more.

Hope this helped!
